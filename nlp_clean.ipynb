{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_clean.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Nj6vPLkY_cI4",
        "h8HVsZbzJS7v",
        "iWkOn47OKEmC",
        "Yu0UcyGI7GHb",
        "9rsV3uGZKwCo",
        "xDshzx0kuap5",
        "GwKW-v7HyqHO",
        "wA02sevwAzaW",
        "QbRLeXSd4gT5",
        "syO4v7Gj4qz6"
      ],
      "authorship_tag": "ABX9TyPga8oMrY5KU5EH74S1Kqwn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csralvall/online_game_toxicity/blob/main/nlp_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsdH2QEr9WkR"
      },
      "source": [
        "# Prelude\n",
        "- Install dependencies.\n",
        "- Import libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFh8HD-mWVnw"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBtxeZnzVXjW"
      },
      "source": [
        "# install dependencies\n",
        "!pip install -U kaggle pip setuptools wheel pandas sklearn numpy spacy nltk gensim fasttext-langdetect wget tqdm mr4mp\n",
        "clear_output()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maXhDvByUuFn"
      },
      "source": [
        "# import libraries\n",
        "from google.colab import drive, files\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pickle\n",
        "import mr4mp\n",
        "import re\n",
        "from spacy.tokens import Token\n",
        "from spacy.language import Language\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import KMeans\n",
        "from ftlangdetect import detect\n",
        "from joblib import Parallel, delayed\n",
        "from google.colab import files\n",
        "from functools import reduce\n",
        "from timeit import default_timer\n",
        "from collections import Counter\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAypiIGA-CLH"
      },
      "source": [
        "- Mount storage.\n",
        "- Load API keys.\n",
        "- Download datasets and store them locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3mjkCTQWaLo",
        "outputId": "538f545b-bc69-464b-dee4-1c2550a78b62"
      },
      "source": [
        "# mount google drive unit to save computationally expensive results\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj6vPLkY_cI4"
      },
      "source": [
        "#### - Only run the following to recreate project from zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZV7XOy9-Vtn"
      },
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "# load Kaggle API keys\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leLnP_RE-oox"
      },
      "source": [
        "# then move kaggle.json into the folder where the API expects to find it\n",
        "# if it is not already present\n",
        "![[ ! -d \"~/.kaggle\" ]] && mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e0ANNpj-u00"
      },
      "source": [
        "# download dataset if not saved in drive\n",
        "![[ ! -f \"/content/gosuai-dota-2-game-chats.zip\" ]] && kaggle datasets download -d romovpa/gosuai-dota-2-game-chats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e02RLlCg-1tP"
      },
      "source": [
        "# unzip file if it is not already inflated\n",
        "![[ ! -f \"/content/dota2_chat_messages.csv\" ]] && unzip gosuai-dota-2-game-chats.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNzufJsk--Hg"
      },
      "source": [
        "# make drive directory if it is not already created\n",
        "%env drive_dir=/content/drive/MyDrive/nlp\n",
        "![[ ! -d $drive_dir ]] && mkdir -p $drive_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP3pR23W_BmR"
      },
      "source": [
        "# move downloaded dataset to drive folder if it is not already there\n",
        "%env drive_file=/content/drive/MyDrive/nlp/dota2_chat_messages.csv\n",
        "![[ ! -f $drive_file ]] && mv dota2_chat_messages.csv $drive_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo0yjMHK_O48"
      },
      "source": [
        "# check google drive folder status\n",
        "!ls /content/drive/MyDrive/nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubc6n3E8_5By"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZo2-u8ZAJYI"
      },
      "source": [
        "# read dataset and create Pandas DataFrame for it\n",
        "df = pd.read_csv('/content/drive/MyDrive/nlp/dota2_chat_messages.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8HVsZbzJS7v"
      },
      "source": [
        "### Anotate language for each row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Ob9u3sATZJ"
      },
      "source": [
        "# create auxiliary function to detect language used in chat message\n",
        "# this function uses a wrapper around the Fasttext model\n",
        "def check_lang(text):\n",
        "    \"\"\"Will return the language corresponding to the\n",
        "    input text\"\"\"\n",
        "    try:\n",
        "        lang = detect(text, low_memory=False)['lang']\n",
        "    except:\n",
        "        lang = \"nal\"\n",
        "\n",
        "    return lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHBQ5Sd7JQiM"
      },
      "source": [
        "# create new column with the detected language\n",
        "# use joblib Parallel function to paralelize detection\n",
        "languages = Parallel(n_jobs=8, verbose=11, backend='multiprocessing', prefer=\"processes\")(\n",
        "    delayed(check_lang)(df.loc[i, \"text\"]) for i in range(0, df.shape[0]))\n",
        "df[\"language\"] = languages\n",
        "df.to_csv(f'/content/drive/MyDrive/nlp/dota2_chat_messages_lang.csv', index=False)\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVdzzM_WJlWU"
      },
      "source": [
        "# get processed dataframe with languages anotated\n",
        "processed_lang = '/content/drive/MyDrive/nlp/dota2_chat_messages_lang.csv'\n",
        "df_lang = pd.read_csv(processed_lang)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6czRjuqXJzSl"
      },
      "source": [
        "# fill null values\n",
        "df_lang = df_lang.fillna(\"\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6e2daIf1Swu"
      },
      "source": [
        "# take only english chats\n",
        "df_nlp = df_lang.loc[df_lang[\"language\"] == \"en\", :].reset_index(drop=True).copy()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWkOn47OKEmC"
      },
      "source": [
        "### Download list of bad words in english (lexicon)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9zeJ-g_KC4j"
      },
      "source": [
        "# get a bad-word list\n",
        "![[ ! -f \"/content/drive/MyDrive/nlp/bad_words.txt\" ]] && wget -O bad_words.txt https://www.cs.cmu.edu/~biglou/resources/bad-words.txt\n",
        "# copy file to drive\n",
        "![[ ! -f \"/content/drive/MyDrive/nlp/bad_words.txt\" ]] && mv /content/bad_words.txt /content/drive/MyDrive/nlp/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu0UcyGI7GHb"
      },
      "source": [
        "### Get bad word list from memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnXy-9AwhltM"
      },
      "source": [
        "# get downloaded bad word list\n",
        "word_list = \"/content/drive/MyDrive/nlp/bad_words.txt\"\n",
        "# use set for fast queries\n",
        "bad_words = set(line.strip() for line in open(word_list, 'r'))\n",
        "# add new bad words\n",
        "bad_words.update(['noob', 'noobs', 'stfu', 'fukign', 'fuking', 'fukin', 'nooob'])\n",
        "bad_dict = dict.fromkeys(bad_words, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rsV3uGZKwCo"
      },
      "source": [
        "### Cleaner function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knuTVnJnKuWR"
      },
      "source": [
        "# clean text from non alphanumeric text to use spacy over clean text\n",
        "def cleaner(df):\n",
        "    \"Extract relevant text from DataFrame using a regex\"\n",
        "    # regex pattern for only alphanumeric, hyphenated text with 3 or more chars\n",
        "    pattern = re.compile(r\"[!A-Za-z0-9\\-]{3,300}\")\n",
        "    df['clean'] = df['text'].str.findall(pattern).str.join(' ')\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds0xP9e42rXk"
      },
      "source": [
        "df_nlp = cleaner(df_nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDshzx0kuap5"
      },
      "source": [
        "### Clean strings and extract features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0XzjNQ15o1b"
      },
      "source": [
        "- Some failed attempts:\n",
        "> The usage of langdetect to detect the language of each chat required at least 12hs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmqd6vLh6CUy"
      },
      "source": [
        "# This cell was created when 'check_lang' used the library langdetect to detect\n",
        "# the language. Langdetect was very slow and required workarounds to avoid the\n",
        "# lose of data when google colab decided to shutdown the instance.\n",
        "# The following code creates a fixed number of disjoint intervals in the range of\n",
        "# the dataset, with a preset step to adjust the interval size.\n",
        "step = df.shape[0]//200\n",
        "print(f'step: {step}')\n",
        "def get_stop(start, step, len):\n",
        "    stop = start + step - 1\n",
        "    if stop > len:\n",
        "        stop = len\n",
        "    return stop\n",
        "ranges = [(start, get_stop(start, step, df.shape[0])) for start in range(0, df.shape[0], step)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6VOQA3b6eOY"
      },
      "source": [
        "# Code cell created to process DataFrame in chunks with langdetect\n",
        "# because it was very slow (~12hs to process whole DataFrame)\n",
        "pd.options.mode.chained_assignment = None\n",
        "for (idx, (start, stop)) in enumerate(ranges):\n",
        "    sub_df = df.iloc[start:stop]\n",
        "    # IMPORTANT: haven't run this but discovered about joblib after doing the task\n",
        "    # with dask. Apparently is faster than dask since there is no graph overhead as in dask\n",
        "    # so from a few tests it seems that it might be faster than dask for about 2 hours.\n",
        "    languages = Parallel(n_jobs=8, verbose=11, backend='multiprocessing', prefer=\"processes\")(\n",
        "        delayed(check_lang)(sub_df.loc[i, \"text\"]) for i in range(start, stop))\n",
        "    sub_df[\"language\"] = languages\n",
        "    sub_df.to_csv(f'/content/drive/MyDrive/nlp/dota2_chat_messages_lang.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG-kcoDku3sJ"
      },
      "source": [
        "# download spacy model for english language\n",
        "!python -m spacy download en_core_web_sm\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isA9eRqiuquO"
      },
      "source": [
        "@Language.component(\"exclamation_flag\")\n",
        "def is_exclamation(doc):\n",
        "    '''\n",
        "        custom component to set flag if token is exclamation sign\n",
        "    '''\n",
        "    exclamation_signs = {token.lemma_: (token.lemma_ == '!') for token in doc}\n",
        "    is_exclamation = lambda x: exclamation_signs[x.lemma_]\n",
        "    Token.set_extension(\"is_exclamation\", getter = is_exclamation, force=True)\n",
        "    return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4sVmxq9uWfV"
      },
      "source": [
        "# removing stop words and unused tokens\n",
        "# use only lemmatizer to get \n",
        "nlp = spacy.load('en_core_web_sm', disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner'])\n",
        "\n",
        "# add custom component to pipeline\n",
        "nlp.add_pipe(\"exclamation_flag\", name=\"exclamation\", last=True)\n",
        "\n",
        "def tokenize(doc):\n",
        "    tokens = list()\n",
        "\n",
        "    for token in doc:\n",
        "        if not token.is_stop and token.is_alpha and len(token) >= 3:\n",
        "            tke = token.text.lower().strip()\n",
        "            tke = re.sub(r'[^a-z0-9\\s]', '', tke)\n",
        "            tokens.append(tke)\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def get_bad_words_score(doc):\n",
        "    bad_words_score = 0\n",
        "    for token in doc:\n",
        "        if token.text in bad_words:\n",
        "            bad_words_score += 1\n",
        "    \n",
        "    return bad_words_score\n",
        "\n",
        "def get_intensity_score(doc):\n",
        "    intensity_score = 0\n",
        "    for token in doc:\n",
        "        if token.is_upper:\n",
        "            intensity_score += 1\n",
        "        if token._.is_exclamation:\n",
        "            intensity_score += 1\n",
        "\n",
        "    return intensity_score\n",
        "\n",
        "# utility functions to paralellize dataprocessing\n",
        "def chunker(iterable, total_length, chunksize):\n",
        "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "    \"Flatten a list of lists to a combined list\"\n",
        "    return [item for sublist in list_of_lists for item in sublist]\n",
        "\n",
        "def process_chunk(texts, function=tokenize):\n",
        "    preproc_pipe = []\n",
        "    for doc in nlp.pipe(texts, batch_size=100):\n",
        "        preproc_pipe.append(function(doc))\n",
        "    return preproc_pipe\n",
        "\n",
        "def preprocess_parallel(texts, processor=process_chunk, chunksize=100):\n",
        "    executor = Parallel(n_jobs=8, backend='multiprocessing', prefer=\"processes\")\n",
        "    do = delayed(processor)\n",
        "    tasks = (do(chunk) for chunk in chunker(texts, len(df_nlp), chunksize=chunksize))\n",
        "    result = executor(tasks)\n",
        "    return flatten(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ESLe9Ubu__f"
      },
      "source": [
        "def wrapper_intensity(text):\n",
        "    return process_chunk(text, get_intensity_score)\n",
        "\n",
        "def wrapper_bad_words(text):\n",
        "    return process_chunk(text, get_bad_words_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7LgZ78LvIiT",
        "outputId": "19d4ae9f-5952-4cd8-86d7-a6780fb58843"
      },
      "source": [
        "# create new column in DataFrame with tokenized words from chat\n",
        "df_nlp['tokens'] = preprocess_parallel(df_nlp['clean'], chunksize=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id9F2ejIvMB1",
        "outputId": "7811e930-2ddb-4035-bb64-d31011f556a0"
      },
      "source": [
        "# create new column in DataFrame with the intensity score\n",
        "df_nlp['intensity'] = preprocess_parallel(df_nlp['clean'], processor=wrapper_intensity, chunksize=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG-lNKpMvQSt",
        "outputId": "9f7bad7b-2863-4476-f6ee-0eee1fb7523b"
      },
      "source": [
        "# create new column in DataFrame with the toxicity score\n",
        "df_nlp['toxicity'] = preprocess_parallel(df_nlp['clean'], processor=wrapper_bad_words, chunksize=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQHpmnsqvWZN"
      },
      "source": [
        "# save anotated DataFrame in memory\n",
        "df_nlp.to_csv(f'/content/drive/MyDrive/nlp/dota2_chat_eng_annotated.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVhIh1OfWkF_"
      },
      "source": [
        "# english chats from original dataset with anotations\n",
        "eng_annotated = '/content/drive/MyDrive/nlp/dota2_chat_eng_annotated.csv'\n",
        "df_eng = pd.read_csv(eng_annotated)\n",
        "df_test = df_eng[:10000]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "8CDodJqzFHJ2",
        "outputId": "ffd59233-4668-4903-83da-94d72d126a7b"
      },
      "source": [
        "df_eng.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>match</th>\n",
              "      <th>time</th>\n",
              "      <th>slot</th>\n",
              "      <th>text</th>\n",
              "      <th>language</th>\n",
              "      <th>clean</th>\n",
              "      <th>tokens</th>\n",
              "      <th>intensity</th>\n",
              "      <th>toxicity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1808.40822</td>\n",
              "      <td>9</td>\n",
              "      <td>100%</td>\n",
              "      <td>en</td>\n",
              "      <td>100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-131.14018</td>\n",
              "      <td>0</td>\n",
              "      <td>twitch.tv/rage_channel</td>\n",
              "      <td>en</td>\n",
              "      <td>twitch rage channel</td>\n",
              "      <td>twitch rage channel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-121.60481</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.twitch.tv/rage_channel</td>\n",
              "      <td>en</td>\n",
              "      <td>https www twitch rage channel</td>\n",
              "      <td>https www twitch rage channel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>700.72893</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.twitch.tv/rage_channel</td>\n",
              "      <td>en</td>\n",
              "      <td>https www twitch rage channel</td>\n",
              "      <td>https www twitch rage channel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>702.99503</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.twitch.tv/rage_channel</td>\n",
              "      <td>en</td>\n",
              "      <td>https www twitch rage channel</td>\n",
              "      <td>https www twitch rage channel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   match        time  slot  ...                         tokens intensity toxicity\n",
              "0      0  1808.40822     9  ...                            NaN         0        0\n",
              "1      1  -131.14018     0  ...            twitch rage channel         0        0\n",
              "2      1  -121.60481     0  ...  https www twitch rage channel         0        0\n",
              "3      1   700.72893     0  ...  https www twitch rage channel         0        0\n",
              "4      1   702.99503     0  ...  https www twitch rage channel         0        0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DNFD0mluFMM0",
        "outputId": "62d3ebef-a8b4-4833-ffd2-9bc1cd55c53e"
      },
      "source": [
        "df_eng.tail()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>match</th>\n",
              "      <th>time</th>\n",
              "      <th>slot</th>\n",
              "      <th>text</th>\n",
              "      <th>language</th>\n",
              "      <th>clean</th>\n",
              "      <th>tokens</th>\n",
              "      <th>intensity</th>\n",
              "      <th>toxicity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6921683</th>\n",
              "      <td>999998</td>\n",
              "      <td>917.21927</td>\n",
              "      <td>8</td>\n",
              "      <td>damn you!!!!</td>\n",
              "      <td>en</td>\n",
              "      <td>damn you!!!!</td>\n",
              "      <td>damn</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6921684</th>\n",
              "      <td>999998</td>\n",
              "      <td>1709.49237</td>\n",
              "      <td>6</td>\n",
              "      <td>baited</td>\n",
              "      <td>en</td>\n",
              "      <td>baited</td>\n",
              "      <td>baited</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6921685</th>\n",
              "      <td>999998</td>\n",
              "      <td>1765.54537</td>\n",
              "      <td>7</td>\n",
              "      <td>lmao</td>\n",
              "      <td>en</td>\n",
              "      <td>lmao</td>\n",
              "      <td>lmao</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6921686</th>\n",
              "      <td>999999</td>\n",
              "      <td>974.04976</td>\n",
              "      <td>0</td>\n",
              "      <td>sec please</td>\n",
              "      <td>en</td>\n",
              "      <td>sec please</td>\n",
              "      <td>sec</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6921687</th>\n",
              "      <td>999999</td>\n",
              "      <td>2674.38856</td>\n",
              "      <td>3</td>\n",
              "      <td>ggwp lol</td>\n",
              "      <td>en</td>\n",
              "      <td>ggwp lol</td>\n",
              "      <td>ggwp lol</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          match        time  slot  ...    tokens intensity toxicity\n",
              "6921683  999998   917.21927     8  ...      damn         4        1\n",
              "6921684  999998  1709.49237     6  ...    baited         0        0\n",
              "6921685  999998  1765.54537     7  ...      lmao         0        0\n",
              "6921686  999999   974.04976     0  ...       sec         0        0\n",
              "6921687  999999  2674.38856     3  ...  ggwp lol         0        0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwKW-v7HyqHO"
      },
      "source": [
        "### Create Bag of Words (BOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wet2ZMgEGbzz"
      },
      "source": [
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6gC4ki9XLqV"
      },
      "source": [
        "# from cleaned english chats get all of them without nan values\n",
        "chats = df_test[['tokens']].dropna().astype(str).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-EYQTPPGFZc"
      },
      "source": [
        "chats = flatten(chats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zoBT44hXDXu"
      },
      "source": [
        "# function to transform chats in sets of words\n",
        "def chat_to_set(chat: [str]) -> {str}:\n",
        "    return set(chat.split())\n",
        "\n",
        "# function to join all chat sets in one big set\n",
        "def join_chat_sets(chat: {str},bag: {str}) -> {str}:\n",
        "    return bag.union(chat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kqLzNLiXdYX",
        "outputId": "6e92307b-091f-4645-9e7d-807dda5d6c85"
      },
      "source": [
        "# use map reduce model to create the Bag of Words (BOW)\n",
        "start = default_timer()\n",
        "pool = mr4mp.pool(10) # roughly 1hs with gpu with full eng dataset\n",
        "set_of_words = pool.mapreduce(chat_to_set, join_chat_sets, chats)\n",
        "pool.close()\n",
        "bag_of_words = dict.fromkeys(set_of_words, 0)\n",
        "print(\"Finished in \" + str(default_timer()-start) + \"s using \" + str(len(pool)) + \" process(es).\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished in 0.45472574099994745s using 10 process(es).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXE51CAMWGTp"
      },
      "source": [
        "# save bag of words in drive (very expensive to compute)\n",
        "# use when running code with full dataset\n",
        "with open('/content/drive/MyDrive/nlp/bag_of_words.pkl', 'wb') as dict_file:\n",
        "    pickle.dump(bag_of_words, dict_file)\n",
        "    dict_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA02sevwAzaW"
      },
      "source": [
        "# Some experiments with bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qebh8reO6F3E",
        "outputId": "ac41dc9a-a4c4-41ff-ec94-6bbec1ac80b8"
      },
      "source": [
        "from nltk import word_tokenize \n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "bigrams = []\n",
        "trigrams = []\n",
        "for line in chats:\n",
        "    token = line.split()\n",
        "    bigrams.append(list(map(lambda x: '_'.join(x), ngrams(token, 2))))\n",
        "    trigrams.append(list(map(lambda x: '_'.join(x), ngrams(token, 3))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUOg2ySv9GcY",
        "outputId": "9db75c80-9493-4cad-b164-4ddb2d64eb4d"
      },
      "source": [
        "len(trigrams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8157"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9OIxBpF91hn"
      },
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "def build_phrases(sentences):\n",
        "    phrases = Phrases(sentences,\n",
        "                      min_count=5,\n",
        "                      threshold=7,\n",
        "                      progress_per=1000)\n",
        "    return Phraser(phrases)\n",
        "\n",
        "bigrams = build_phrases(chats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c6L5zQeAJLm"
      },
      "source": [
        "bigrams = list(map(lambda x: [x], bigrams[chats]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLFtyd44-oeL"
      },
      "source": [
        "w2v_bi = generate_embedding(bigrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9psITMZ-uni",
        "outputId": "33e2ae40-f7e0-431e-eb17-087cf04d83c5"
      },
      "source": [
        "w2v_bi.wv.most_similar('report')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('nice', 0.2529045641422272),\n",
              " ('carry', 0.20082908868789673),\n",
              " ('ggwp', 0.17018888890743256),\n",
              " ('team', 0.15016482770442963),\n",
              " ('mid', 0.13887985050678253),\n",
              " ('feed', 0.10852647572755814),\n",
              " ('guys', 0.09936434030532837),\n",
              " ('commend', 0.03476494550704956),\n",
              " ('def', 0.0330718494951725),\n",
              " ('win', 0.019886532798409462)]"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbRLeXSd4gT5"
      },
      "source": [
        "# Generate word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OsRlXlT7zs7"
      },
      "source": [
        "# function to create embeddings of words in each chat\n",
        "def generate_embedding(sentences: [[str]]) -> ([str], np.ndarray):\n",
        "  w2v_model = Word2Vec(\n",
        "                     min_count=20,\n",
        "                     window=2,\n",
        "                     #size=300,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     workers=1)\n",
        "\n",
        "  w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "  w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "\n",
        "  return w2v_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8RRrUE-5gyt"
      },
      "source": [
        "vocab = list(map(lambda x: x.split(), chats))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCC4MAJ63sPm"
      },
      "source": [
        "w2v_model = generate_embedding(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syO4v7Gj4qz6"
      },
      "source": [
        "# Create clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaomdggXZjci"
      },
      "source": [
        "bow_serie = []\n",
        "for idx, chat in enumerate(chats):\n",
        "    bow = dict.fromkeys(set_of_words, 0)\n",
        "    bad_dict = dict.fromkeys(bad_words, 0)\n",
        "    word_list = chat.split()\n",
        "    prod = np.ones(100)\n",
        "    for word in word_list:\n",
        "        if word in w2v_model.wv:\n",
        "            prod *= w2v_model.wv[word] \n",
        "        if word in bad_dict:\n",
        "            bad_dict[word] += 1\n",
        "        if word in bow:\n",
        "            bow[word] += 1\n",
        "    bad_array = np.fromiter(bad_dict.values(), dtype=int)\n",
        "    bag_array = np.fromiter(bow.values(), dtype=int)\n",
        "    chat_embed = np.concatenate((prod, bad_array))\n",
        "    chat_array = np.concatenate((bad_array, bag_array))\n",
        "    np.append(chat_array, df_test.loc[idx, ['intensity']])\n",
        "    np.append(chat_embed, df_test.loc[idx, ['intensity']])\n",
        "    if idx == 0:\n",
        "        bow_serie = chat_array\n",
        "        embd_serie = chat_embed\n",
        "    else:\n",
        "        bow_serie = np.concatenate((bow_serie, chat_array))\n",
        "        embd_serie = np.concatenate((embd_serie, chat_embed))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CzQQY-Osoj7"
      },
      "source": [
        "bow_length = len(set_of_words) + len(bad_words)\n",
        "bow_serie = bow_serie.reshape((bow_length, -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkKKX1zA9ne3"
      },
      "source": [
        "embd_length = 100 + len(bad_words)\n",
        "embd_serie = embd_serie.reshape((embd_length, -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHrScPAEwEr2"
      },
      "source": [
        "bow_matrix = bow_serie[~np.isnan(bow_serie)].reshape((bow_length, -1))\n",
        "embd_matrix = embd_serie[~np.isnan(embd_serie)].reshape((embd_length, -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVZBvVQj6EWR"
      },
      "source": [
        "def reduce_matrix(matrix: np.ndarray, *, variance_treshold: float):\n",
        "    print(f'INPUT SHAPE: {matrix.shape}')\n",
        "    # reduce all vectors to [0, 1] space\n",
        "    normalized_matrix = normalize(matrix, axis=1)\n",
        "    # compute variances in each row\n",
        "    matrix_variances = np.var(matrix, axis=0)\n",
        "    # create mask for features with high correlation (low variance)\n",
        "    bool_mask = np.where(matrix_variances < variance_treshold)\n",
        "    # filter features with high correlation (variance under treshold)\n",
        "    raked_matrix = np.delete(normalized_matrix, bool_mask, axis=1)\n",
        "    print(f'OUTPUT SHAPE: {raked_matrix.shape}')\n",
        "    return raked_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLHhdTn56Ywf",
        "outputId": "8c795daf-e35b-41e0-c54a-a00ffe10cdcb"
      },
      "source": [
        "bow_reduced = reduce_matrix(bow_matrix, variance_treshold=0.0001)\n",
        "embd_reduced = reduce_matrix(embd_matrix, variance_treshold=0.0215)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT SHAPE: (5227, 8157)\n",
            "OUTPUT SHAPE: (5227, 6866)\n",
            "INPUT SHAPE: (1491, 8157)\n",
            "OUTPUT SHAPE: (1491, 7964)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu2EI9Ty6osz"
      },
      "source": [
        "def generate_clusters(\n",
        "    matrix: np.ndarray,\n",
        "    n_clusters: int\n",
        ") -> KMeans:\n",
        "    # generate word clusters using the KMeans algorithm.\n",
        "    print(\"\\nClustering started\")\n",
        "    # Instantiate KMeans clusterer for n_clusters\n",
        "    km_model = KMeans(n_clusters=n_clusters, random_state=3)\n",
        "    # create clusters\n",
        "    km_model.fit(matrix)\n",
        "    print(\"Clustering finished\")\n",
        "    return km_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPhX4J3Z69Hx",
        "outputId": "9e3aa936-a38d-49fc-ec9e-684d4ee2d7d0"
      },
      "source": [
        "bow_clusters = generate_clusters(bow_serie, 50)\n",
        "embd_clusters = generate_clusters(embd_serie, 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Clustering started\n",
            "Clustering finished\n",
            "\n",
            "Clustering started\n",
            "Clustering finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m5VxwFj7pvE"
      },
      "source": [
        "def display_summary(clusters: KMeans):\n",
        "    cluster_count = Counter(sorted(clusters.labels_))\n",
        "    for cluster in cluster_count:\n",
        "        print (\"Cluster#\", cluster,\" - Total words:\", cluster_count[cluster])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSr-_Yk87viz",
        "outputId": "bb9c7995-8517-41c3-e224-755fe1b8b3a2"
      },
      "source": [
        "# show number of words captured by each cluster\n",
        "display_summary(embd_clusters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster# 0  - Total words: 16\n",
            "Cluster# 1  - Total words: 33\n",
            "Cluster# 2  - Total words: 382\n",
            "Cluster# 3  - Total words: 13\n",
            "Cluster# 4  - Total words: 23\n",
            "Cluster# 5  - Total words: 22\n",
            "Cluster# 6  - Total words: 29\n",
            "Cluster# 7  - Total words: 24\n",
            "Cluster# 8  - Total words: 20\n",
            "Cluster# 9  - Total words: 14\n",
            "Cluster# 10  - Total words: 38\n",
            "Cluster# 11  - Total words: 19\n",
            "Cluster# 12  - Total words: 25\n",
            "Cluster# 13  - Total words: 18\n",
            "Cluster# 14  - Total words: 10\n",
            "Cluster# 15  - Total words: 15\n",
            "Cluster# 16  - Total words: 22\n",
            "Cluster# 17  - Total words: 27\n",
            "Cluster# 18  - Total words: 18\n",
            "Cluster# 19  - Total words: 17\n",
            "Cluster# 20  - Total words: 34\n",
            "Cluster# 21  - Total words: 21\n",
            "Cluster# 22  - Total words: 21\n",
            "Cluster# 23  - Total words: 13\n",
            "Cluster# 24  - Total words: 16\n",
            "Cluster# 25  - Total words: 23\n",
            "Cluster# 26  - Total words: 18\n",
            "Cluster# 27  - Total words: 23\n",
            "Cluster# 28  - Total words: 30\n",
            "Cluster# 29  - Total words: 9\n",
            "Cluster# 30  - Total words: 18\n",
            "Cluster# 31  - Total words: 24\n",
            "Cluster# 32  - Total words: 52\n",
            "Cluster# 33  - Total words: 23\n",
            "Cluster# 34  - Total words: 27\n",
            "Cluster# 35  - Total words: 28\n",
            "Cluster# 36  - Total words: 25\n",
            "Cluster# 37  - Total words: 29\n",
            "Cluster# 38  - Total words: 29\n",
            "Cluster# 39  - Total words: 13\n",
            "Cluster# 40  - Total words: 8\n",
            "Cluster# 41  - Total words: 25\n",
            "Cluster# 42  - Total words: 20\n",
            "Cluster# 43  - Total words: 23\n",
            "Cluster# 44  - Total words: 26\n",
            "Cluster# 45  - Total words: 24\n",
            "Cluster# 46  - Total words: 29\n",
            "Cluster# 47  - Total words: 19\n",
            "Cluster# 48  - Total words: 34\n",
            "Cluster# 49  - Total words: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p5KqB3lo15J",
        "outputId": "18efdbdf-c6f4-4990-b54b-e9651c7b28fe"
      },
      "source": [
        "display_summary(bow_clusters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster# 0  - Total words: 1\n",
            "Cluster# 1  - Total words: 5070\n",
            "Cluster# 2  - Total words: 5\n",
            "Cluster# 3  - Total words: 1\n",
            "Cluster# 4  - Total words: 1\n",
            "Cluster# 5  - Total words: 1\n",
            "Cluster# 6  - Total words: 1\n",
            "Cluster# 7  - Total words: 1\n",
            "Cluster# 8  - Total words: 1\n",
            "Cluster# 9  - Total words: 1\n",
            "Cluster# 10  - Total words: 6\n",
            "Cluster# 11  - Total words: 1\n",
            "Cluster# 12  - Total words: 5\n",
            "Cluster# 13  - Total words: 6\n",
            "Cluster# 14  - Total words: 1\n",
            "Cluster# 15  - Total words: 1\n",
            "Cluster# 16  - Total words: 5\n",
            "Cluster# 17  - Total words: 1\n",
            "Cluster# 18  - Total words: 1\n",
            "Cluster# 19  - Total words: 1\n",
            "Cluster# 20  - Total words: 4\n",
            "Cluster# 21  - Total words: 3\n",
            "Cluster# 22  - Total words: 1\n",
            "Cluster# 23  - Total words: 1\n",
            "Cluster# 24  - Total words: 7\n",
            "Cluster# 25  - Total words: 1\n",
            "Cluster# 26  - Total words: 4\n",
            "Cluster# 27  - Total words: 1\n",
            "Cluster# 28  - Total words: 1\n",
            "Cluster# 29  - Total words: 6\n",
            "Cluster# 30  - Total words: 7\n",
            "Cluster# 31  - Total words: 7\n",
            "Cluster# 32  - Total words: 1\n",
            "Cluster# 33  - Total words: 1\n",
            "Cluster# 34  - Total words: 1\n",
            "Cluster# 35  - Total words: 1\n",
            "Cluster# 36  - Total words: 11\n",
            "Cluster# 37  - Total words: 5\n",
            "Cluster# 38  - Total words: 1\n",
            "Cluster# 39  - Total words: 1\n",
            "Cluster# 40  - Total words: 1\n",
            "Cluster# 41  - Total words: 5\n",
            "Cluster# 42  - Total words: 1\n",
            "Cluster# 43  - Total words: 7\n",
            "Cluster# 44  - Total words: 8\n",
            "Cluster# 45  - Total words: 6\n",
            "Cluster# 46  - Total words: 10\n",
            "Cluster# 47  - Total words: 6\n",
            "Cluster# 48  - Total words: 6\n",
            "Cluster# 49  - Total words: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "628xHBsKvEXw"
      },
      "source": [
        "df_test = df_test.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oN0JpYkuRdp"
      },
      "source": [
        "def annotate_dataframe(clusters: KMeans, df: pd.DataFrame, col_name: str):\n",
        "    cluster_count = Counter(sorted(clusters.labels_))\n",
        "    #sort cluster centers by proximity to centroid\n",
        "    order_centroids = clusters.cluster_centers_.argsort()[:, ::-1] \n",
        "\n",
        "    clusters_df = np.zeros(len(df))\n",
        "    \n",
        "    for cluster_idx in cluster_count:\n",
        "        # get words inside each cluster\n",
        "        cluster_words = np.where(clusters.labels_ == cluster_idx)[0]\n",
        "        # anotate all chats in cluster\n",
        "        for idx in cluster_words:\n",
        "            clusters_df[idx] = int(cluster_idx)\n",
        "\n",
        "    df[col_name] = clusters_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5adupRW8v7vk"
      },
      "source": [
        "annotate_dataframe(bow_clusters, df_test, 'bow_clusters')\n",
        "annotate_dataframe(embd_clusters, df_test, 'embd_clusters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7jOXlqFw_Lu"
      },
      "source": [
        "bow_group = df_test.groupby('bow_clusters')\n",
        "embd_group = df_test.groupby('embd_clusters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wJe46lwxrMw",
        "outputId": "0a1eb8fc-957e-4112-b5c4-0cb407314333"
      },
      "source": [
        "bow_group.get_group(36)['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "278     If I roll a one\n",
              "1225    lol i win game \n",
              "1323          comend me\n",
              "1766       unbelievable\n",
              "2132        Forgot game\n",
              "2174              fast \n",
              "2726              gg wp\n",
              "3276               HELP\n",
              "3481                 XD\n",
              "4650            fuckers\n",
              "4910              GG WP\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtpwGTryiitB",
        "outputId": "16cc588f-32a0-41fd-ad49-82a556f912ab"
      },
      "source": [
        "embd_group.get_group(38)['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54                 free farming ls\n",
              "139              for farming a lot\n",
              "175                 happy new year\n",
              "192            passive heross here\n",
              "226                           lmao\n",
              "277           its our midlaner lol\n",
              "294                             ff\n",
              "381                 guys wtf is up\n",
              "415                            lol\n",
              "466                         archon\n",
              "483                          right\n",
              "500                           ggwp\n",
              "517               u just like bara\n",
              "568     Worst country of the world\n",
              "585         what does the rat say?\n",
              "689                            me?\n",
              "723                Merry Christmas\n",
              "757                          i win\n",
              "774                     these guys\n",
              "876            16 kills still lose\n",
              "929                  and feels bad\n",
              "946        or do you want gay porn\n",
              "997              must chat all now\n",
              "1014                           Dog\n",
              "1048                           ???\n",
              "1135      true sight is 100 longer\n",
              "1203                   small favor\n",
              "1220           impossible game gg \n",
              "1288                       AND ALL\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtgbl_8idr6e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}