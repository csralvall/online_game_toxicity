{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOXActmNwXtCQk7EyLHcAYx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csralvall/online_game_toxicity/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0qnO29b8k-K"
      },
      "source": [
        "#### Import util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFh8HD-mWVnw"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdLL1fw88qRM"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBtxeZnzVXjW"
      },
      "source": [
        "!pip install -U pip setuptools wheel pandas spacy fasttext-langdetect wget\n",
        "clear_output()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu514gDN8vEU"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maXhDvByUuFn"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from spacy.tokens import Token\n",
        "from spacy.language import Language\n",
        "from ftlangdetect import detect\n",
        "from joblib import Parallel, delayed"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_sRSKON81c7"
      },
      "source": [
        "### Mount storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3mjkCTQWaLo",
        "outputId": "c81e1d62-7058-4fd3-e763-cf85067e8073"
      },
      "source": [
        "# mount google drive unit to save computationally expensive results\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PqKTyl3zpdI"
      },
      "source": [
        "### Load dataset from disk to memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZo2-u8ZAJYI"
      },
      "source": [
        "# read dataset and create Pandas DataFrame for it\n",
        "df = pd.read_csv('/content/drive/MyDrive/nlp/dota2_chat_messages.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8HVsZbzJS7v"
      },
      "source": [
        "### Anotate language for each row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0XzjNQ15o1b"
      },
      "source": [
        "- Some failed attempts:\n",
        "> The usage of langdetect to detect the language of each chat required at least 12hs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmqd6vLh6CUy"
      },
      "source": [
        "# This cell was created when 'check_lang' used the library langdetect to detect\n",
        "# the language. Langdetect was very slow and required workarounds to avoid the\n",
        "# lose of data when google colab decided to shutdown the instance.\n",
        "# The following code creates a fixed number of disjoint intervals in the range of\n",
        "# the dataset, with a preset step to adjust the interval size.\n",
        "step = df.shape[0]//200\n",
        "print(f'step: {step}')\n",
        "def get_stop(start, step, len):\n",
        "    stop = start + step - 1\n",
        "    if stop > len:\n",
        "        stop = len\n",
        "    return stop\n",
        "ranges = [(start, get_stop(start, step, df.shape[0])) for start in range(0, df.shape[0], step)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6VOQA3b6eOY"
      },
      "source": [
        "# Code cell created to process DataFrame in chunks with langdetect\n",
        "# because it was very slow (~12hs to process whole DataFrame)\n",
        "pd.options.mode.chained_assignment = None\n",
        "for (idx, (start, stop)) in enumerate(ranges):\n",
        "    sub_df = df.iloc[start:stop]\n",
        "    # IMPORTANT: haven't run this but discovered about joblib after doing the task\n",
        "    # with dask. Apparently is faster than dask since there is no graph overhead as in dask\n",
        "    # so from a few tests it seems that it might be faster than dask for about 2 hours.\n",
        "    languages = Parallel(n_jobs=8, verbose=11, backend='multiprocessing', prefer=\"processes\")(\n",
        "        delayed(check_lang)(sub_df.loc[i, \"text\"]) for i in range(start, stop))\n",
        "    sub_df[\"language\"] = languages\n",
        "    sub_df.to_csv(f'/content/drive/MyDrive/nlp/dota2_chat_messages_lang.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fes5C1fiTqiF"
      },
      "source": [
        "--------------------------\n",
        "#### Successful case:\n",
        "> Using fasttext\n",
        "--------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Ob9u3sATZJ"
      },
      "source": [
        "# create auxiliary function to detect language used in chat message\n",
        "# this function uses a wrapper around the Fasttext model\n",
        "def check_lang(text):\n",
        "    \"\"\"Will return the language corresponding to the\n",
        "    input text\"\"\"\n",
        "    try:\n",
        "        lang = detect(text, low_memory=False)['lang']\n",
        "    except:\n",
        "        lang = \"nal\"\n",
        "\n",
        "    return lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHBQ5Sd7JQiM"
      },
      "source": [
        "# create new column with the detected language\n",
        "# use joblib Parallel function to paralelize detection\n",
        "languages = Parallel(n_jobs=8, verbose=11, backend='multiprocessing', prefer=\"processes\")(\n",
        "    delayed(check_lang)(df.loc[i, \"text\"]) for i in range(0, df.shape[0]))\n",
        "df[\"language\"] = languages\n",
        "df.to_csv(f'/content/drive/MyDrive/nlp/dota2_chat_messages_lang.csv', index=False)\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHaHyjR7Awas"
      },
      "source": [
        "#### Load dataset with annotated languages from storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVdzzM_WJlWU"
      },
      "source": [
        "# get processed dataframe with languages anotated\n",
        "processed_lang = '/content/drive/MyDrive/nlp/dota2_chat_messages_lang.csv'\n",
        "df_lang = pd.read_csv(processed_lang)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6czRjuqXJzSl"
      },
      "source": [
        "# fill null values\n",
        "df_lang = df_lang.fillna(\"\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6e2daIf1Swu"
      },
      "source": [
        "# take only english chats\n",
        "df_nlp = df_lang.loc[df_lang[\"language\"] == \"en\", :].reset_index(drop=True).copy()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu0UcyGI7GHb"
      },
      "source": [
        "### Get bad word list from memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnXy-9AwhltM"
      },
      "source": [
        "# get downloaded bad word list\n",
        "word_list = \"/content/drive/MyDrive/nlp/bad_words.txt\"\n",
        "# use set for fast queries\n",
        "bad_words = set(line.strip() for line in open(word_list, 'r'))\n",
        "# add new bad words\n",
        "bad_words.update(['noob', 'noobs', 'stfu', 'fukign', 'fuking', 'fukin', 'nooob'])\n",
        "bad_dict = dict.fromkeys(bad_words, 0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rsV3uGZKwCo"
      },
      "source": [
        "### Cleaner function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knuTVnJnKuWR"
      },
      "source": [
        "# clean text from non alphanumeric text to use spacy over clean text\n",
        "def cleaner(df):\n",
        "    \"Extract relevant text from DataFrame using a regex\"\n",
        "    # regex pattern for only alphanumeric, hyphenated text with 3 or more chars\n",
        "    pattern = re.compile(r\"[!A-Za-z0-9\\-]{3,300}\")\n",
        "    df['clean'] = df['text'].str.findall(pattern).str.join(' ')\n",
        "    return df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds0xP9e42rXk"
      },
      "source": [
        "df_nlp = cleaner(df_nlp)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDshzx0kuap5"
      },
      "source": [
        "### Clean strings and extract features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG-kcoDku3sJ"
      },
      "source": [
        "# download spacy model for english language\n",
        "!python -m spacy download en_core_web_sm\n",
        "clear_output()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isA9eRqiuquO"
      },
      "source": [
        "@Language.component(\"exclamation_flag\")\n",
        "def is_exclamation(doc):\n",
        "    '''\n",
        "        custom component to set flag if token is exclamation sign\n",
        "    '''\n",
        "    exclamation_signs = {token.lemma_: (token.lemma_ == '!') for token in doc}\n",
        "    is_exclamation = lambda x: exclamation_signs[x.lemma_]\n",
        "    Token.set_extension(\"is_exclamation\", getter = is_exclamation, force=True)\n",
        "    return doc"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4sVmxq9uWfV"
      },
      "source": [
        "# removing stop words and unused tokens\n",
        "nlp = spacy.load('en_core_web_sm', disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner'])\n",
        "\n",
        "# add custom component to pipeline\n",
        "nlp.add_pipe(\"exclamation_flag\", name=\"exclamation\", last=True)\n",
        "\n",
        "def process(doc):\n",
        "    tokens = list()\n",
        "\n",
        "    for token in doc:\n",
        "        if not token.is_stop and token.is_alpha and len(token) >= 3:\n",
        "            tke = token.text.lower().strip()\n",
        "            tke = re.sub(r'[^a-z0-9\\s]', '', tke)\n",
        "            tokens.append(tke)\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def get_bad_words_score(doc):\n",
        "    bad_words_score = 0\n",
        "    for token in doc:\n",
        "        if token.text in bad_words:\n",
        "            bad_words_score += 1\n",
        "    \n",
        "    return bad_words_score\n",
        "\n",
        "def get_intensity_score(doc):\n",
        "    intensity_score = 0\n",
        "    for token in doc:\n",
        "        if token.is_upper:\n",
        "            intensity_score += 1\n",
        "        if token._.is_exclamation:\n",
        "            intensity_score += 1\n",
        "\n",
        "    return intensity_score\n",
        "\n",
        "# utility functions to paralellize dataprocessing\n",
        "def chunker(iterable, total_length, chunksize):\n",
        "    \" Return a generator of chunks from iterable\"\n",
        "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "    \"Flatten a list of lists to a combined list\"\n",
        "    return [item for sublist in list_of_lists for item in sublist]\n",
        "\n",
        "def process_chunk(texts, function=process):\n",
        "    preproc_pipe = []\n",
        "    for doc in nlp.pipe(texts, batch_size=100):\n",
        "        preproc_pipe.append(function(doc))\n",
        "    return preproc_pipe\n",
        "\n",
        "def preprocess_parallel(texts, processor=process_chunk, chunksize=100):\n",
        "    executor = Parallel(n_jobs=8, backend='multiprocessing', prefer=\"processes\")\n",
        "    do = delayed(processor)\n",
        "    tasks = (do(chunk) for chunk in chunker(texts, len(df_nlp), chunksize=chunksize))\n",
        "    result = executor(tasks)\n",
        "    return flatten(result)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ESLe9Ubu__f"
      },
      "source": [
        "def wrapper_intensity(text):\n",
        "    return process_chunk(text, get_intensity_score)\n",
        "\n",
        "def wrapper_bad_words(text):\n",
        "    return process_chunk(text, get_bad_words_score)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7LgZ78LvIiT"
      },
      "source": [
        "# create new column in DataFrame with tokenized words from chat\n",
        "df_nlp['tokens'] = preprocess_parallel(df_nlp['clean'], chunksize=1000)\n",
        "clear_output()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id9F2ejIvMB1"
      },
      "source": [
        "# create new column in DataFrame with the intensity score\n",
        "df_nlp['intensity'] = preprocess_parallel(df_nlp['clean'], processor=wrapper_intensity, chunksize=1000)\n",
        "clear_output()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG-lNKpMvQSt"
      },
      "source": [
        "# create new column in DataFrame with the toxicity score\n",
        "df_nlp['toxicity'] = preprocess_parallel(df_nlp['clean'], processor=wrapper_bad_words, chunksize=1000)\n",
        "clear_output()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQHpmnsqvWZN"
      },
      "source": [
        "# save anotated DataFrame in memory\n",
        "df_nlp.to_csv(f'/content/drive/MyDrive/nlp/dota2_chat_eng_annotated.csv', index=False)"
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}